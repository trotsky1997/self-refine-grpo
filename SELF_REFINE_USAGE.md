# Self-Refine Agent Usage Guide

## æ¶æ„æ¦‚è§ˆ

Self-refine åŠŸèƒ½ç°å·²æ¨¡å—åŒ–ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„å­ agentï¼š

```
SelfRefineAgent (self_refine_agent.py)
â”œâ”€â”€ Answer Agent    - ç”Ÿæˆåˆå§‹/æ”¹è¿›ç­”æ¡ˆ
â”œâ”€â”€ Critique Agent  - è¯„ä¼°å¹¶æ‰¹è¯„å½“å‰ç­”æ¡ˆ
â””â”€â”€ Refine Agent    - åŸºäºæ‰¹è¯„æ”¹è¿›ç­”æ¡ˆ
```

## æ–‡ä»¶ç»“æ„

- **`self_refine_agent.py`** - Self-refine agent ç±»å®šä¹‰
  - ç‹¬ç«‹çš„ä¸‰ä¸ªå­ agent æ–¹æ³•
  - æ”¯æŒå•è½®å’Œå¤šè½®æ”¹è¿›æ¥å£
  
- **`trainer.py`** - GRPO trainerï¼ˆå·²ç®€åŒ–ï¼‰
  - å¯¼å…¥å¹¶ä½¿ç”¨ `SelfRefineAgent`
  - åœ¨ `_generate_and_score_completions` ä¸­è°ƒç”¨ agent æ–¹æ³•

- **`grpo_vlm.py`** - é…ç½®å’Œå¯åŠ¨
  - è®¾ç½® `max_refine_rounds` å‚æ•°

## ä½¿ç”¨æ–¹å¼

### 1. åŸºç¡€é…ç½®ï¼ˆå•è½®æ”¹è¿›ï¼‰

```python
trainer = GRPOTrainer(
    ...,
    enable_self_refine=True,      # å¯ç”¨ self-refine
    use_critique=True,             # å¯ç”¨ä¸¤é˜¶æ®µ refineï¼ˆcritique + refineï¼‰
    max_refine_rounds=1,           # å•è½®æ”¹è¿›
)
```

### 2. å¤šè½®æ”¹è¿›ï¼ˆæœªæ¥æ‰©å±•ï¼‰

```python
trainer = GRPOTrainer(
    ...,
    enable_self_refine=True,
    use_critique=True,
    max_refine_rounds=3,  # ä¸‰è½®è¿­ä»£æ”¹è¿›
)
```

## Agent å·¥ä½œæµç¨‹

### Stage 1: Critique Agent

```
ä¸æ­£ç¡®æ ·æœ¬ â†’ Critique Agent â†’ ç”Ÿæˆæ‰¹è¯„
```

**è¾“å…¥**ï¼š
- åŸå§‹é—®é¢˜ prompts
- ä¸æ­£ç¡®çš„ç­”æ¡ˆ

**è¾“å‡º**ï¼š
- ç»“æ„åŒ–æ‰¹è¯„ï¼ˆè¯„ä¼°æ¨ç†ã€è®¡ç®—ã€å®Œæ•´æ€§ï¼‰

### Stage 2: Refine Agent

```
åŸå§‹é—®é¢˜ + åŸå§‹ç­”æ¡ˆ + æ‰¹è¯„ â†’ Refine Agent â†’ ç”Ÿæˆæ”¹è¿›ç­”æ¡ˆ
```

**è¾“å…¥**ï¼š
- åŸå§‹é—®é¢˜ prompts
- ä¸æ­£ç¡®çš„ç­”æ¡ˆ
- æ‰¹è¯„å†…å®¹

**è¾“å‡º**ï¼š
- æ”¹è¿›åçš„ç­”æ¡ˆ

## æç¤ºè¯æ¨¡æ¿

### Critique Agent System Prompt

```
You are a critical reviewer. Evaluate the given solution and provide constructive feedback.

Focus on:
- Correctness of reasoning and calculations
- Completeness of the solution
- Clarity and organization
- Common mistakes or edge cases

Format your critique in <think></think> tags for internal analysis, 
followed by clear, actionable feedback.
```

### Refine Agent System Prompt

```
You are a problem solver. Given a question and feedback on a previous attempt, 
provide an improved solution.

Use the standard format:
<think>Your unstructured internal reasoning</think>
<answer>Clear, well-formatted solution with final answer in \boxed{}</answer>
```

## ç»Ÿä¸€æ¥å£è®¾è®¡ â­

### æ ¸å¿ƒåŸåˆ™

**trainer.py åªè°ƒç”¨ä¸€æ¬¡ï¼Œagent å†…éƒ¨å¤„ç†æ‰€æœ‰å·¥ä½œæµç¨‹ï¼**

è¿™æ ·è®¾è®¡çš„å¥½å¤„ï¼š
1. **å®Œå…¨é»‘ç›’** - Agent å†…éƒ¨å¤„ç†ä¸€åˆ‡ï¼Œtrainer ä¸çŸ¥é“ä»»ä½•ç»†èŠ‚
2. **æ˜“æ‰©å±•** - æœªæ¥æ·»åŠ å…¶ä»– agent åªéœ€å®ç°ç›¸åŒæ¥å£
3. **æç®€** - trainer ä»£ç åªéœ€ä¼ å…¥ inputsï¼Œæ¥æ”¶ outputs

### ç»Ÿä¸€æ¥å£

**å”¯ä¸€è°ƒç”¨ç‚¹ - ä¸€æ¬¡æå®šæ‰€æœ‰ï¼**
```python
# â­ ULTIMATE UNIFIED INTERFACE â­
# Single call - agent handles EVERYTHING:
# - Critique generation
# - Refine prompt building  
# - Regeneration
# - Evaluation
# - Logging
refined_result = agent.refine_batch(
    inputs=inputs,
    original_result=result,
    original_prompts=prompts,
    answers=completions_text,
    solutions=solutions,
    correctness_mask=correctness_tensor,
    batch_counter=batch_counter
)
```

**Agent å†…éƒ¨è‡ªåŠ¨å®Œæˆ**ï¼š
- âœ… Critique ç”Ÿæˆ
- âœ… Refine prompt æ„å»º
- âœ… Prompt å±•å¼€åˆ° full batch
- âœ… è°ƒç”¨ generationï¼ˆå†…éƒ¨ï¼‰
- âœ… è§£ç  completionsï¼ˆå†…éƒ¨ï¼‰
- âœ… è¯„ä¼°ç»Ÿè®¡ï¼ˆå†…éƒ¨ï¼‰
- âœ… æ—¥å¿—è®°å½•ï¼ˆå†…éƒ¨ï¼‰
- âœ… ç»“æœæ‰“å°ï¼ˆå†…éƒ¨ï¼‰
- âœ… è¿”å› refined result

## ä»£ç æ”¹è¿›

### ä¹‹å‰ï¼ˆæš´éœ²æ‰€æœ‰å†…éƒ¨ç»†èŠ‚ï¼‰

```python
# trainer.py: 300+ è¡Œæ‰‹åŠ¨å¤„ç†æ‰€æœ‰é€»è¾‘

# 1. æ‰“å°ç»Ÿè®¡
print(f"Batch size: {batch_size}")
print(f"Accuracy@t1: {batch_acc_t1:.2f}%")
# ... æ›´å¤šç»Ÿè®¡æ‰“å°

# 2. æ‰‹åŠ¨æ„å»º critique prompts
for idx, inp in enumerate(inputs):
    if needs_refine_mask[idx]:
        critique_prompt = []
        for i, msg in enumerate(original_prompt):
            # ... 100+ è¡Œæ„å»ºé€»è¾‘

# 3. ç”Ÿæˆ critique
critique_result = super()._generate_and_score_completions(...)

# 4. æ‰‹åŠ¨æ„å»º refine prompts
for idx, inp in enumerate(inputs):
    if needs_refine_mask[idx]:
        refine_prompt = []
        # ... 100+ è¡Œæ„å»ºé€»è¾‘

# 5. æ‰‹åŠ¨è¯„ä¼°
for idx in range(len(completions_text)):
    is_correct = self._check_if_correct(...)
    # ... ç»Ÿè®¡ transitions

# 6. æ‰‹åŠ¨æ‰“å°ç»“æœ
print(f"Accuracy@t2: {batch_acc_t2:.2f}%")
# ... æ›´å¤šè¾“å‡º

# 7. æ‰‹åŠ¨è®°å½•æ—¥å¿—
for idx in range(len(completions_text)):
    log_entry = {...}
    self._log_refine_sample(log_entry)
```

### ä¹‹åï¼ˆå®Œå…¨é»‘ç›’ï¼‰

```python
# trainer.py: ä»… ~5 è¡Œï¼ğŸ‰

# â­ ä¸€æ¬¡è°ƒç”¨ï¼Œæå®šæ‰€æœ‰ï¼
refined_result = self.refine_agent.refine_batch(
    inputs=inputs,
    original_result=result,
    original_prompts=prompts,
    answers=completions_text,
    solutions=solutions,
    correctness_mask=correctness_tensor,
    batch_counter=self.batch_counter
)

# Done! ğŸ‰
# Agent å†…éƒ¨è‡ªåŠ¨å¤„ç†ï¼š
# - Critique ç”Ÿæˆ
# - Refine prompts æ„å»º
# - Prompt å±•å¼€åˆ° full batch
# - è°ƒç”¨ generation
# - è§£ç  completions
# - è¯„ä¼°ç»Ÿè®¡ï¼ˆaccuracyã€transitionsï¼‰
# - ç»“æœæ‰“å°ï¼ˆæ ¼å¼åŒ–è¾“å‡ºï¼‰
# - æ—¥å¿—è®°å½•ï¼ˆJSONL æ–‡ä»¶ï¼‰
# - è¿”å› refined result

# Trainer å®Œå…¨ä¸çŸ¥é“å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼âœ¨
```

**ä»£ç è¡Œæ•°å‡å°‘**ï¼š300+ è¡Œ â†’ 5 è¡Œï¼ˆ**-98%ï¼**ï¼‰

## æ—¥å¿—è¾“å‡º

è®­ç»ƒæ—¶ä¼šçœ‹åˆ°ç”± agent æ§åˆ¶çš„ç®€æ´è¾“å‡ºï¼š

```
[Self-Refine Agent] Processing 4 incorrect samples using Self-Refine Agent (Critique + Refine)

ğŸ“Š Self-Refine Agent Evaluation (Batch 1):
  Accuracy@t1: 4/8 = 50.00%
  Accuracy@t2: 6/8 = 75.00%
  Î”(batch):    +25.00% ğŸ“ˆ

  Transitions:
    iâ†’c:  2  |  iâ†’i:  2
    câ†’c:  4  |  câ†’i:  0

  Example trace:
    Original: To solve this problem...
    Critique: The reasoning is correct but...
    Refined:  Using the corrected approach...
    Result: âœ“ Correct

  âœ“ Logged 8 samples to self_refine_log.jsonl
```

**å…³é”®æ”¹è¿›**ï¼š
- âœ… æ‰€æœ‰è¾“å‡ºç”± agent æ§åˆ¶ï¼Œä¸æš´éœ²ç»™ trainer
- âœ… æ ¼å¼ç»Ÿä¸€ï¼Œæ˜“äºè§£æ
- âœ… åŒ…å«å®Œæ•´çš„è¯„ä¼°ç»Ÿè®¡
- âœ… è‡ªåŠ¨è®°å½•åˆ° JSONL æ–‡ä»¶

## æ‰©å±•å¤šè½®æ”¹è¿›

å½“å‰å®ç°æ”¯æŒå•è½®æ”¹è¿›ã€‚è¦å¯ç”¨å¤šè½®è¿­ä»£ï¼š

1. åœ¨ `grpo_vlm.py` ä¸­è®¾ç½® `max_refine_rounds > 1`
2. Agent ä¼šè‡ªåŠ¨è¿­ä»£å¤šè½®ï¼ˆcritique â†’ refine â†’ critique â†’ refine...ï¼‰
3. æ¯è½®ç»“æœéƒ½ä¼šè®°å½•åˆ° `refine_history`

```python
# æœªæ¥ï¼šå¤šè½®æ”¹è¿›ä¼šè‡ªåŠ¨æ‰§è¡Œ
for round in range(max_refine_rounds):
    critiques = critique_agent(...)
    refined_answers = refine_agent(..., critiques, round_idx=round)
    # è¯„ä¼°å¹¶å†³å®šæ˜¯å¦ç»§ç»­
```

## æ‰©å±•å…¶ä»–é‡‡æ · Agent

ç»Ÿä¸€æ¥å£è®¾è®¡ä½¿å¾—æ·»åŠ æ–°çš„é‡‡æ · agent å˜å¾—å®¹æ˜“ï¼š

### æ­¥éª¤ 1: åˆ›å»ºæ–° Agent ç±»

```python
# my_custom_agent.py
class MyCustomSamplingAgent:
    """è‡ªå®šä¹‰é‡‡æ · agent"""
    
    def __init__(self, trainer, **kwargs):
        self.trainer = trainer
        # ... åˆå§‹åŒ–å‚æ•°
    
    # â­ å®ç°ç»Ÿä¸€æ¥å£ï¼ˆä¸€æ¬¡è°ƒç”¨ï¼‰
    def refine_batch(
        self,
        inputs,
        original_result,
        original_prompts,
        answers,
        solutions,
        correctness_mask,
        batch_counter=0
    ):
        """
        ç»Ÿä¸€æ¥å£ï¼šå¤„ç†æ•´ä¸ª batchï¼Œè¿”å› refined result
        
        Agent å†…éƒ¨è‡ªä¸»å¤„ç†ï¼š
        - ç”Ÿæˆã€è¯„ä¼°ã€æ—¥å¿—
        """
        # 1. æå– incorrect samplesï¼ˆå†…éƒ¨ï¼‰
        incorrect_indices = torch.where(~correctness_mask)[0].tolist()
        if not incorrect_indices:
            return None
        
        # 2. ä½ çš„è‡ªå®šä¹‰é‡‡æ ·é€»è¾‘ï¼ˆå†…éƒ¨ï¼‰
        # - å¯ä»¥ç”¨å¼ºåŒ–å­¦ä¹ 
        # - å¯ä»¥ç”¨ beam search
        # - å¯ä»¥ç”¨å¤šè½®è¿­ä»£
        refined_prompts = self._your_custom_sampling(...)
        
        # 3. å±•å¼€åˆ° full batchï¼ˆå†…éƒ¨ï¼‰
        refine_inputs = self._expand_to_full_batch(...)
        
        # 4. è°ƒç”¨ç”Ÿæˆï¼ˆå†…éƒ¨ï¼‰
        refined_result = _GRPOTrainer._generate_and_score_completions(
            self.trainer, refine_inputs
        )
        
        # 5. è¯„ä¼°å’Œæ—¥å¿—ï¼ˆå†…éƒ¨ï¼‰
        self._evaluate_and_log(...)
        
        # 6. è¿”å›ç»“æœ
        return refined_result
```

### æ­¥éª¤ 2: åœ¨ trainer.py ä¸­åˆ‡æ¢ Agent

```python
# trainer.py __init__
if self.enable_self_refine:
    if agent_type == "self_refine":
        self.refine_agent = SelfRefineAgent(self, max_refine_rounds)
    elif agent_type == "custom":
        self.refine_agent = MyCustomSamplingAgent(self, **kwargs)
    # ... æ›´å¤š agent ç±»å‹
```

### æ­¥éª¤ 3: æ— éœ€ä¿®æ”¹è°ƒç”¨ä»£ç 

```python
# trainer.py _generate_and_score_completions
# æ— è®ºä»€ä¹ˆ agentï¼Œè°ƒç”¨æ–¹å¼å®Œå…¨ç›¸åŒï¼â­
refined_result = self.refine_agent.refine_batch(
    inputs=inputs,
    original_result=result,
    original_prompts=prompts,
    answers=completions_text,
    solutions=solutions,
    correctness_mask=correctness_tensor,
    batch_counter=self.batch_counter
)
```

**å…³é”®ç‚¹**ï¼š
- âœ… æ–° agent åªéœ€å®ç° `refine_batch()` æ¥å£ï¼ˆä¸€æ¬¡è°ƒç”¨ï¼Œè¿”å›ç»“æœï¼‰
- âœ… trainer.py **å®Œå…¨æ— éœ€ä¿®æ”¹**ï¼Œåªæ˜¯ä¼ å…¥æ•°æ®ï¼Œæ¥æ”¶ç»“æœ
- âœ… Agent å†…éƒ¨è‡ªä¸»å†³å®šï¼šç”Ÿæˆã€è¯„ä¼°ã€æ—¥å¿—
- âœ… å¯ä»¥è‡ªç”±åˆ‡æ¢ä¸åŒ agent
- âœ… ä¿æŒä»£ç æç®€ï¼ˆ5 è¡Œï¼‰å’Œå¯ç»´æŠ¤æ€§

## ä¼˜åŠ¿æ€»ç»“

| æŒ‡æ ‡ | ä¹‹å‰ | ä¹‹å | æ”¹è¿› |
|------|------|------|------|
| **trainer.py ä»£ç è¡Œæ•°** | 300+ è¡Œ | **5 è¡Œ** | **-98%** ğŸ‰ |
| **trainer éœ€è¦çš„è°ƒç”¨æ¬¡æ•°** | æ‰‹åŠ¨æ„å»º prompts + ç”Ÿæˆ + è§£ç  + è¯„ä¼° + æ—¥å¿— | **ä»… 1 æ¬¡** | **æç®€** |
| **æš´éœ²çš„å†…éƒ¨ç»†èŠ‚** | critique/refine/è¯„ä¼°/æ—¥å¿—/ç”Ÿæˆ/è§£ç  | **æ— ** | **å®Œå…¨é»‘ç›’** âœ¨ |
| **trainer æ˜¯å¦çŸ¥é“ agent åšäº†ä»€ä¹ˆ** | æ˜¯ï¼ˆæ‰‹åŠ¨å¤„ç†æ‰€æœ‰æ­¥éª¤ï¼‰ | **å¦** | **å®Œå…¨å°è£…** |
| **æ—¥å¿—æ§åˆ¶** | trainer | agent | **agent è‡ªä¸»** |
| **è¯„ä¼°æ§åˆ¶** | trainer | agent | **agent è‡ªä¸»** |
| **ç”Ÿæˆæ§åˆ¶** | trainer | agent | **agent è‡ªä¸»** |
| **æ¥å£å¤æ‚åº¦** | å¤šæ­¥éª¤æ‰‹åŠ¨è°ƒç”¨ | **å•æ¬¡è°ƒç”¨** | **æç®€** |

### æ ¸å¿ƒä¼˜åŠ¿

1. **å®Œå…¨é»‘ç›’** â­â­â­ - Trainer å®Œå…¨ä¸çŸ¥é“ agent å†…éƒ¨åšäº†ä»€ä¹ˆï¼Œåªæ˜¯ä¼ å…¥æ•°æ®ï¼Œç­‰å¾…ç»“æœ
2. **æç®€æ¥å£** - Trainer åªéœ€ **1 æ¬¡è°ƒç”¨**ï¼Œä»£ç é‡å‡å°‘ **98%**
3. **å¯æ‰©å±•** - è½»æ¾æ·»åŠ æ–°çš„ agentï¼Œåªéœ€å®ç° `refine_batch()` æ¥å£
4. **å¯æµ‹è¯•** - Agent å¯ç‹¬ç«‹æµ‹è¯•ï¼Œtrainer æ— éœ€å…³å¿ƒä»»ä½•ç»†èŠ‚
5. **çµæ´»** - æ”¯æŒå•è½®/å¤šè½®ï¼Œæœ‰/æ—  critiqueï¼Œagent å†…éƒ¨å†³å®š
6. **å¯æ’æ‹”** - ä¸åŒ agent å¯è‡ªç”±åˆ‡æ¢ï¼Œæ— éœ€ä¿®æ”¹ trainer
7. **è‡ªä¸»æ§åˆ¶** - Agent å†³å®šä¸€åˆ‡ï¼šç”Ÿæˆã€è¯„ä¼°ã€æ—¥å¿—ã€æ‰“å°
8. **çœŸæ­£çš„ä»£ç†æ¨¡å¼** - Trainer åªæ˜¯å§”æ‰˜ä»»åŠ¡ï¼Œagent å®Œå…¨è‡ªä¸»æ‰§è¡Œ

## æ–‡ä»¶æ¸…ç†

å·²åˆ é™¤å†—ä½™æ–‡ä»¶ï¼š
- `benchmark_gspo.py`
- `check_optimizations.py`
- `test_lora_triton.py`
- `test_triton_integration.py`
- `test.sh`
- `agentic-sampler.py`

ä¿ç•™æ ¸å¿ƒæ–‡ä»¶ï¼š
- `self_refine_agent.py` â­ æ–°å¢ï¼šAgent å°è£…
- `sampler.py` â­ æ–°å¢ï¼šSampler è§£è€¦
- `trainer.py` âœ… ç®€åŒ–
- `grpo_vlm.py` âœ… é…ç½®
- `launch.sh` âœ… å¯åŠ¨

è¯¦ç»†æ¶æ„æ–‡æ¡£ï¼š
- `SELF_REFINE_ARCHITECTURE.md` - Agent æ¶æ„
- `SAMPLER_ARCHITECTURE.md` - Sampler æ¶æ„
